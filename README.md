# ğŸ—ï¸ Automated End-to-End Data Warehouse Pipeline

## ğŸ“Œ Overview
This project demonstrates a **production-style Data Engineering pipeline** that builds and maintains a Data Warehouse using Python and PostgreSQL.

The system automatically:
- Initializes database schemas & tables
- Loads raw CSV data into staging
- Transforms data into Star Schema
- Performs incremental loading
- Updates analytics tables
- Runs automatically using Linux Cron

No manual database setup is required.

---

## ğŸ§± Architecture

---

## âš™ï¸ Tech Stack

- Python
- PostgreSQL
- Pandas
- psycopg2
- Linux (WSL Ubuntu)
- Git & GitHub
- Cron Scheduler

---

## ğŸ“‚ Project Structure

---

## ğŸ”„ Pipeline Workflow

1. Auto-create schemas and tables (if missing)
2. Detect last processed record
3. Load only new data (Incremental Load)
4. Populate dimension tables
5. Update fact table
6. Refresh analytics layer
7. Write execution logs

---

## âœ… Key Features

- âœ… Automated database initialization
- âœ… Incremental ETL processing
- âœ… Production-safe path handling
- âœ… Logging system
- âœ… Cron-based automation
- âœ… Clone & Run setup

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone repository

### 2ï¸âƒ£ Create virtual environment

### 3ï¸âƒ£ Install dependencies

### 4ï¸âƒ£ Configure environment variables

Create `.env` file:

### 5ï¸âƒ£ Run pipeline

### 5ï¸âƒ£ Run pipeline

Database and tables will be created automatically.

---

## â±ï¸ Automation (Cron)

Example cron job:

Runs pipeline daily at 2 AM.

---

## ğŸ“Š Example Output

- Updated warehouse tables
- Monthly revenue analytics
- Execution logs inside `/logs`

---

## ğŸ¯ Learning Outcomes

This project demonstrates:

- Data Warehouse Design
- ETL Pipeline Engineering
- Incremental Data Loading
- Automation & Scheduling
- Deployment across systems
- Production-style project structure

---

## ğŸ‘¨â€ğŸ’» Author

Dhruv Kesarwani  
Aspiring Data Engineer ğŸš€
